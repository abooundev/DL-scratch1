{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 설명\n",
    "| 파일명 | 파일 용도 | 관련 절 | 페이지 |\n",
    "|:--   |:--      |:--    |:--      |\n",
    "| batch_norm_gradient_check.py | 배치 정규화를 구현한 신경망의 오차역전파법 방식의 기울기 계산이 정확한지 확인합니다(기울기 확인). |  |  |\n",
    "| batch_norm_test.py | MNIST 데이터셋 학습에 배치 정규화를 적용해봅니다. | 6.3.2 배치 정규화의 효과 | 212 |\n",
    "| hyperparameter_optimization.py | 무작위로 추출한 값부터 시작하여 두 하이퍼파라미터(가중치 감소 계수, 학습률)를 최적화해봅니다. | 6.5.3 하이퍼파라미터 최적화 구현하기 | 224 |\n",
    "| optimizer_compare_mnist.py | SGD, 모멘텀, AdaGrad, Adam의 학습 속도를 비교합니다. | 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교 | 201 |\n",
    "| optimizer_compare_naive.py | SGD, 모멘텀, AdaGrad, Adam의 학습 패턴을 비교합니다. | 6.1.7 어느 갱신 방법을 이용할 것인가? | 200 |\n",
    "| overfit_dropout.py | 일부러 오버피팅을 일으킨 후 드롭아웃(dropout)의 효과를 관찰합니다. | 6.4.3 드롭아웃 | 219 |\n",
    "| overfit_weight_decay.py | 일부러 오버피팅을 일으킨 후 가중치 감소(weight_decay)의 효과를 관찰합니다. | 6.4.1 오버피팅 | 215 |\n",
    "| weight_init_activation_histogram.py | 활성화 함수로 시그모이드 함수를 사용하는 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 그려봅니다. | 6.2.2 은닉층의 활성화값 분포 | 203 |\n",
    "| weight_init_compare.py | 가중치 초깃값(std=0.01, He, Xavier)에 따른 학습 속도를 비교합니다. | 6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교 | 209 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_norm_gradient_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0.0\n",
      "b1:0.0\n",
      "gamma1:0.0\n",
      "beta1:0.0\n",
      "W2:0.0\n",
      "b2:0.0\n",
      "gamma2:0.0\n",
      "beta2:0.04636255680514623\n",
      "W3:0.0\n",
      "b3:1.7990402263745597e-07\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100], output_size=10,\n",
    "                              use_batchnorm=True)\n",
    "\n",
    "x_batch = x_train[:1]\n",
    "t_batch = t_train[:1]\n",
    "\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "W1:0.0\n",
    "b1:0.0\n",
    "gamma1:0.0\n",
    "beta1:0.0\n",
    "W2:0.0\n",
    "b2:0.0\n",
    "gamma2:0.0\n",
    "beta2:0.04636255680514623\n",
    "W3:0.0\n",
    "b3:1.7990402263745597e-07\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_norm_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 1/16 ==============\n",
      "epoch:0 | 0.093 - 0.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csg/HomeWorkspace/git/DL-scratch1/common/layers.py:12: RuntimeWarning: invalid value encountered in less_equal\n",
      "  self.mask = (x <= 0)\n",
      "/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: overflow encountered in square\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
      "/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | 0.097 - 0.107\n",
      "epoch:2 | 0.097 - 0.137\n",
      "epoch:3 | 0.097 - 0.165\n",
      "epoch:4 | 0.097 - 0.183\n",
      "epoch:5 | 0.097 - 0.205\n",
      "epoch:6 | 0.097 - 0.227\n",
      "epoch:7 | 0.097 - 0.247\n",
      "epoch:8 | 0.097 - 0.27\n",
      "epoch:9 | 0.097 - 0.287\n",
      "epoch:10 | 0.097 - 0.309\n",
      "epoch:11 | 0.097 - 0.324\n",
      "epoch:12 | 0.097 - 0.345\n",
      "epoch:13 | 0.097 - 0.345\n",
      "epoch:14 | 0.097 - 0.367\n",
      "epoch:15 | 0.097 - 0.384\n",
      "epoch:16 | 0.097 - 0.387\n",
      "epoch:17 | 0.097 - 0.407\n",
      "epoch:18 | 0.097 - 0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.097 - 0.421\n",
      "============== 2/16 ==============\n",
      "epoch:0 | 0.094 - 0.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csg/HomeWorkspace/git/DL-scratch1/common/layers.py:12: RuntimeWarning: invalid value encountered in less_equal\n",
      "  self.mask = (x <= 0)\n",
      "/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: overflow encountered in square\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
      "/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | 0.097 - 0.088\n",
      "epoch:2 | 0.097 - 0.095\n",
      "epoch:3 | 0.097 - 0.118\n",
      "epoch:4 | 0.097 - 0.142\n",
      "epoch:5 | 0.097 - 0.151\n",
      "epoch:6 | 0.097 - 0.192\n",
      "epoch:7 | 0.097 - 0.21\n",
      "epoch:8 | 0.097 - 0.228\n",
      "epoch:9 | 0.097 - 0.251\n",
      "epoch:10 | 0.097 - 0.271\n",
      "epoch:11 | 0.097 - 0.3\n",
      "epoch:12 | 0.097 - 0.322\n",
      "epoch:13 | 0.097 - 0.348\n",
      "epoch:14 | 0.097 - 0.361\n",
      "epoch:15 | 0.097 - 0.392\n",
      "epoch:16 | 0.097 - 0.401\n",
      "epoch:17 | 0.097 - 0.422\n",
      "epoch:18 | 0.097 - 0.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.097 - 0.459\n",
      "============== 3/16 ==============\n",
      "epoch:0 | 0.146 - 0.078\n",
      "epoch:1 | 0.305 - 0.103\n",
      "epoch:2 | 0.461 - 0.149\n",
      "epoch:3 | 0.558 - 0.173\n",
      "epoch:4 | 0.633 - 0.193\n",
      "epoch:5 | 0.673 - 0.231\n",
      "epoch:6 | 0.743 - 0.258\n",
      "epoch:7 | 0.777 - 0.298\n",
      "epoch:8 | 0.822 - 0.332\n",
      "epoch:9 | 0.848 - 0.364\n",
      "epoch:10 | 0.866 - 0.399\n",
      "epoch:11 | 0.885 - 0.44\n",
      "epoch:12 | 0.902 - 0.471\n",
      "epoch:13 | 0.919 - 0.507\n",
      "epoch:14 | 0.927 - 0.525\n",
      "epoch:15 | 0.934 - 0.543\n",
      "epoch:16 | 0.939 - 0.57\n",
      "epoch:17 | 0.953 - 0.586\n",
      "epoch:18 | 0.961 - 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.966 - 0.626\n",
      "============== 4/16 ==============\n",
      "epoch:0 | 0.098 - 0.117\n",
      "epoch:1 | 0.226 - 0.135\n",
      "epoch:2 | 0.378 - 0.183\n",
      "epoch:3 | 0.469 - 0.248\n",
      "epoch:4 | 0.545 - 0.31\n",
      "epoch:5 | 0.607 - 0.369\n",
      "epoch:6 | 0.64 - 0.439\n",
      "epoch:7 | 0.664 - 0.481\n",
      "epoch:8 | 0.713 - 0.519\n",
      "epoch:9 | 0.729 - 0.571\n",
      "epoch:10 | 0.761 - 0.59\n",
      "epoch:11 | 0.766 - 0.63\n",
      "epoch:12 | 0.781 - 0.655\n",
      "epoch:13 | 0.802 - 0.678\n",
      "epoch:14 | 0.809 - 0.698\n",
      "epoch:15 | 0.823 - 0.72\n",
      "epoch:16 | 0.84 - 0.734\n",
      "epoch:17 | 0.839 - 0.757\n",
      "epoch:18 | 0.852 - 0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.858 - 0.78\n",
      "============== 5/16 ==============\n",
      "epoch:0 | 0.115 - 0.102\n",
      "epoch:1 | 0.126 - 0.143\n",
      "epoch:2 | 0.13 - 0.266\n",
      "epoch:3 | 0.143 - 0.411\n",
      "epoch:4 | 0.159 - 0.5\n",
      "epoch:5 | 0.174 - 0.579\n",
      "epoch:6 | 0.183 - 0.628\n",
      "epoch:7 | 0.186 - 0.681\n",
      "epoch:8 | 0.196 - 0.702\n",
      "epoch:9 | 0.201 - 0.736\n",
      "epoch:10 | 0.206 - 0.766\n",
      "epoch:11 | 0.211 - 0.789\n",
      "epoch:12 | 0.215 - 0.809\n",
      "epoch:13 | 0.221 - 0.826\n",
      "epoch:14 | 0.222 - 0.835\n",
      "epoch:15 | 0.237 - 0.844\n",
      "epoch:16 | 0.236 - 0.861\n",
      "epoch:17 | 0.238 - 0.872\n",
      "epoch:18 | 0.25 - 0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.247 - 0.881\n",
      "============== 6/16 ==============\n",
      "epoch:0 | 0.093 - 0.128\n",
      "epoch:1 | 0.116 - 0.271\n",
      "epoch:2 | 0.135 - 0.424\n",
      "epoch:3 | 0.122 - 0.545\n",
      "epoch:4 | 0.116 - 0.639\n",
      "epoch:5 | 0.116 - 0.707\n",
      "epoch:6 | 0.116 - 0.762\n",
      "epoch:7 | 0.116 - 0.786\n",
      "epoch:8 | 0.116 - 0.812\n",
      "epoch:9 | 0.116 - 0.84\n",
      "epoch:10 | 0.116 - 0.858\n",
      "epoch:11 | 0.116 - 0.876\n",
      "epoch:12 | 0.116 - 0.888\n",
      "epoch:13 | 0.116 - 0.903\n",
      "epoch:14 | 0.116 - 0.911\n",
      "epoch:15 | 0.116 - 0.919\n",
      "epoch:16 | 0.116 - 0.925\n",
      "epoch:17 | 0.116 - 0.93\n",
      "epoch:18 | 0.116 - 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 0.944\n",
      "============== 7/16 ==============\n",
      "epoch:0 | 0.097 - 0.163\n",
      "epoch:1 | 0.117 - 0.271\n",
      "epoch:2 | 0.117 - 0.589\n",
      "epoch:3 | 0.117 - 0.678\n",
      "epoch:4 | 0.117 - 0.734\n",
      "epoch:5 | 0.117 - 0.769\n",
      "epoch:6 | 0.117 - 0.813\n",
      "epoch:7 | 0.117 - 0.841\n",
      "epoch:8 | 0.117 - 0.872\n",
      "epoch:9 | 0.117 - 0.894\n",
      "epoch:10 | 0.117 - 0.917\n",
      "epoch:11 | 0.117 - 0.935\n",
      "epoch:12 | 0.117 - 0.945\n",
      "epoch:13 | 0.117 - 0.957\n",
      "epoch:14 | 0.117 - 0.966\n",
      "epoch:15 | 0.117 - 0.969\n",
      "epoch:16 | 0.117 - 0.975\n",
      "epoch:17 | 0.117 - 0.978\n",
      "epoch:18 | 0.117 - 0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.985\n",
      "============== 8/16 ==============\n",
      "epoch:0 | 0.1 - 0.136\n",
      "epoch:1 | 0.117 - 0.4\n",
      "epoch:2 | 0.116 - 0.66\n",
      "epoch:3 | 0.116 - 0.753\n",
      "epoch:4 | 0.116 - 0.796\n",
      "epoch:5 | 0.116 - 0.843\n",
      "epoch:6 | 0.116 - 0.863\n",
      "epoch:7 | 0.116 - 0.881\n",
      "epoch:8 | 0.116 - 0.907\n",
      "epoch:9 | 0.116 - 0.927\n",
      "epoch:10 | 0.116 - 0.953\n",
      "epoch:11 | 0.116 - 0.97\n",
      "epoch:12 | 0.116 - 0.981\n",
      "epoch:13 | 0.116 - 0.992\n",
      "epoch:14 | 0.116 - 0.992\n",
      "epoch:15 | 0.116 - 0.994\n",
      "epoch:16 | 0.116 - 0.996\n",
      "epoch:17 | 0.116 - 0.996\n",
      "epoch:18 | 0.116 - 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 0.997\n",
      "============== 9/16 ==============\n",
      "epoch:0 | 0.099 - 0.135\n",
      "epoch:1 | 0.116 - 0.616\n",
      "epoch:2 | 0.116 - 0.705\n",
      "epoch:3 | 0.116 - 0.771\n",
      "epoch:4 | 0.116 - 0.853\n",
      "epoch:5 | 0.116 - 0.927\n",
      "epoch:6 | 0.116 - 0.956\n",
      "epoch:7 | 0.116 - 0.971\n",
      "epoch:8 | 0.116 - 0.985\n",
      "epoch:9 | 0.116 - 0.988\n",
      "epoch:10 | 0.116 - 0.991\n",
      "epoch:11 | 0.116 - 0.997\n",
      "epoch:12 | 0.116 - 0.976\n",
      "epoch:13 | 0.116 - 0.999\n",
      "epoch:14 | 0.116 - 1.0\n",
      "epoch:15 | 0.116 - 1.0\n",
      "epoch:16 | 0.116 - 1.0\n",
      "epoch:17 | 0.116 - 1.0\n",
      "epoch:18 | 0.116 - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 1.0\n",
      "============== 10/16 ==============\n",
      "epoch:0 | 0.087 - 0.119\n",
      "epoch:1 | 0.117 - 0.537\n",
      "epoch:2 | 0.117 - 0.67\n",
      "epoch:3 | 0.116 - 0.746\n",
      "epoch:4 | 0.116 - 0.762\n",
      "epoch:5 | 0.116 - 0.764\n",
      "epoch:6 | 0.116 - 0.8\n",
      "epoch:7 | 0.116 - 0.801\n",
      "epoch:8 | 0.116 - 0.807\n",
      "epoch:9 | 0.116 - 0.847\n",
      "epoch:10 | 0.116 - 0.865\n",
      "epoch:11 | 0.116 - 0.871\n",
      "epoch:12 | 0.116 - 0.935\n",
      "epoch:13 | 0.116 - 0.939\n",
      "epoch:14 | 0.116 - 0.966\n",
      "epoch:15 | 0.116 - 0.977\n",
      "epoch:16 | 0.117 - 0.987\n",
      "epoch:17 | 0.117 - 0.989\n",
      "epoch:18 | 0.117 - 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 0.991\n",
      "============== 11/16 ==============\n",
      "epoch:0 | 0.094 - 0.119\n",
      "epoch:1 | 0.116 - 0.5\n",
      "epoch:2 | 0.117 - 0.675\n",
      "epoch:3 | 0.117 - 0.653\n",
      "epoch:4 | 0.117 - 0.705\n",
      "epoch:5 | 0.117 - 0.762\n",
      "epoch:6 | 0.117 - 0.725\n",
      "epoch:7 | 0.116 - 0.808\n",
      "epoch:8 | 0.116 - 0.871\n",
      "epoch:9 | 0.116 - 0.761\n",
      "epoch:10 | 0.116 - 0.933\n",
      "epoch:11 | 0.116 - 0.915\n",
      "epoch:12 | 0.116 - 0.973\n",
      "epoch:13 | 0.116 - 0.968\n",
      "epoch:14 | 0.116 - 0.981\n",
      "epoch:15 | 0.116 - 0.953\n",
      "epoch:16 | 0.116 - 0.985\n",
      "epoch:17 | 0.116 - 0.988\n",
      "epoch:18 | 0.116 - 0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 0.995\n",
      "============== 12/16 ==============\n",
      "epoch:0 | 0.1 - 0.109\n",
      "epoch:1 | 0.117 - 0.471\n",
      "epoch:2 | 0.117 - 0.621\n",
      "epoch:3 | 0.117 - 0.669\n",
      "epoch:4 | 0.117 - 0.671\n",
      "epoch:5 | 0.117 - 0.672\n",
      "epoch:6 | 0.117 - 0.753\n",
      "epoch:7 | 0.117 - 0.754\n",
      "epoch:8 | 0.117 - 0.737\n",
      "epoch:9 | 0.117 - 0.767\n",
      "epoch:10 | 0.117 - 0.739\n",
      "epoch:11 | 0.117 - 0.787\n",
      "epoch:12 | 0.117 - 0.788\n",
      "epoch:13 | 0.117 - 0.798\n",
      "epoch:14 | 0.117 - 0.845\n",
      "epoch:15 | 0.117 - 0.866\n",
      "epoch:16 | 0.117 - 0.911\n",
      "epoch:17 | 0.117 - 0.965\n",
      "epoch:18 | 0.117 - 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.986\n",
      "============== 13/16 ==============\n",
      "epoch:0 | 0.1 - 0.197\n",
      "epoch:1 | 0.117 - 0.486\n",
      "epoch:2 | 0.117 - 0.519\n",
      "epoch:3 | 0.116 - 0.574\n",
      "epoch:4 | 0.116 - 0.577\n",
      "epoch:5 | 0.116 - 0.653\n",
      "epoch:6 | 0.117 - 0.68\n",
      "epoch:7 | 0.116 - 0.7\n",
      "epoch:8 | 0.117 - 0.678\n",
      "epoch:9 | 0.117 - 0.703\n",
      "epoch:10 | 0.117 - 0.744\n",
      "epoch:11 | 0.117 - 0.804\n",
      "epoch:12 | 0.117 - 0.799\n",
      "epoch:13 | 0.117 - 0.737\n",
      "epoch:14 | 0.117 - 0.807\n",
      "epoch:15 | 0.117 - 0.805\n",
      "epoch:16 | 0.117 - 0.806\n",
      "epoch:17 | 0.117 - 0.812\n",
      "epoch:18 | 0.117 - 0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.814\n",
      "============== 14/16 ==============\n",
      "epoch:0 | 0.1 - 0.1\n",
      "epoch:1 | 0.116 - 0.261\n",
      "epoch:2 | 0.116 - 0.464\n",
      "epoch:3 | 0.116 - 0.38\n",
      "epoch:4 | 0.116 - 0.479\n",
      "epoch:5 | 0.116 - 0.489\n",
      "epoch:6 | 0.117 - 0.474\n",
      "epoch:7 | 0.116 - 0.472\n",
      "epoch:8 | 0.116 - 0.503\n",
      "epoch:9 | 0.116 - 0.491\n",
      "epoch:10 | 0.116 - 0.504\n",
      "epoch:11 | 0.116 - 0.471\n",
      "epoch:12 | 0.116 - 0.516\n",
      "epoch:13 | 0.116 - 0.474\n",
      "epoch:14 | 0.116 - 0.453\n",
      "epoch:15 | 0.116 - 0.484\n",
      "epoch:16 | 0.116 - 0.481\n",
      "epoch:17 | 0.116 - 0.437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 | 0.117 - 0.443\n",
      "epoch:19 | 0.117 - 0.411\n",
      "============== 15/16 ==============\n",
      "epoch:0 | 0.116 - 0.122\n",
      "epoch:1 | 0.116 - 0.301\n",
      "epoch:2 | 0.116 - 0.407\n",
      "epoch:3 | 0.117 - 0.409\n",
      "epoch:4 | 0.117 - 0.413\n",
      "epoch:5 | 0.117 - 0.495\n",
      "epoch:6 | 0.117 - 0.481\n",
      "epoch:7 | 0.117 - 0.507\n",
      "epoch:8 | 0.117 - 0.513\n",
      "epoch:9 | 0.117 - 0.519\n",
      "epoch:10 | 0.117 - 0.509\n",
      "epoch:11 | 0.117 - 0.511\n",
      "epoch:12 | 0.117 - 0.52\n",
      "epoch:13 | 0.117 - 0.519\n",
      "epoch:14 | 0.117 - 0.493\n",
      "epoch:15 | 0.117 - 0.492\n",
      "epoch:16 | 0.117 - 0.506\n",
      "epoch:17 | 0.117 - 0.507\n",
      "epoch:18 | 0.117 - 0.499\n",
      "epoch:19 | 0.117 - 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 16/16 ==============\n",
      "epoch:0 | 0.092 - 0.156\n",
      "epoch:1 | 0.117 - 0.233\n",
      "epoch:2 | 0.116 - 0.378\n",
      "epoch:3 | 0.117 - 0.41\n",
      "epoch:4 | 0.117 - 0.438\n",
      "epoch:5 | 0.117 - 0.426\n",
      "epoch:6 | 0.117 - 0.43\n",
      "epoch:7 | 0.117 - 0.421\n",
      "epoch:8 | 0.117 - 0.43\n",
      "epoch:9 | 0.117 - 0.431\n",
      "epoch:10 | 0.117 - 0.395\n",
      "epoch:11 | 0.117 - 0.421\n",
      "epoch:12 | 0.117 - 0.53\n",
      "epoch:13 | 0.117 - 0.513\n",
      "epoch:14 | 0.117 - 0.507\n",
      "epoch:15 | 0.117 - 0.508\n",
      "epoch:16 | 0.117 - 0.529\n",
      "epoch:17 | 0.117 - 0.533\n",
      "epoch:18 | 0.117 - 0.614\n",
      "epoch:19 | 0.117 - 0.621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.optimizer import SGD, Adam\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 학습 데이터를 줄임\n",
    "x_train = x_train[:1000]\n",
    "t_train = t_train[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def __train(weight_init_std):\n",
    "    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n",
    "                                    weight_init_std=weight_init_std, use_batchnorm=True)\n",
    "    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n",
    "                                weight_init_std=weight_init_std)\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "    \n",
    "    train_acc_list = []\n",
    "    bn_train_acc_list = []\n",
    "    \n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    epoch_cnt = 0\n",
    "    \n",
    "    for i in range(1000000000):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "    \n",
    "        for _network in (bn_network, network):\n",
    "            grads = _network.gradient(x_batch, t_batch)\n",
    "            optimizer.update(_network.params, grads)\n",
    "    \n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            bn_train_acc = bn_network.accuracy(x_train, t_train)\n",
    "            train_acc_list.append(train_acc)\n",
    "            bn_train_acc_list.append(bn_train_acc)\n",
    "    \n",
    "            print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n",
    "    \n",
    "            epoch_cnt += 1\n",
    "            if epoch_cnt >= max_epochs:\n",
    "                break\n",
    "                \n",
    "    return train_acc_list, bn_train_acc_list\n",
    "\n",
    "\n",
    "# 그래프 그리기==========\n",
    "weight_scale_list = np.logspace(0, -4, num=16)\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "for i, w in enumerate(weight_scale_list):\n",
    "    print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
    "    train_acc_list, bn_train_acc_list = __train(w)\n",
    "    \n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.title(\"W:\" + str(w))\n",
    "    if i == 15:\n",
    "        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
    "    else:\n",
    "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
    "\n",
    "    plt.ylim(0, 1.0)\n",
    "    if i % 4:\n",
    "        plt.yticks([])\n",
    "    else:\n",
    "        plt.ylabel(\"accuracy\")\n",
    "    if i < 12:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel(\"epochs\")\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n============== 1/16 ==============\\nepoch:0 | 0.093 - 0.094\\n/Users/csg/HomeWorkspace/git/DL-scratch1/common/layers.py:12: RuntimeWarning: invalid value encountered in less_equal\\n  self.mask = (x <= 0)\\n/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: overflow encountered in square\\n  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\\n/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: invalid value encountered in double_scalars\\n  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\\nepoch:1 | 0.097 - 0.107\\nepoch:2 | 0.097 - 0.137\\nepoch:3 | 0.097 - 0.165\\nepoch:4 | 0.097 - 0.183\\nepoch:5 | 0.097 - 0.205\\nepoch:6 | 0.097 - 0.227\\nepoch:7 | 0.097 - 0.247\\nepoch:8 | 0.097 - 0.27\\nepoch:9 | 0.097 - 0.287\\nepoch:10 | 0.097 - 0.309\\nepoch:11 | 0.097 - 0.324\\nepoch:12 | 0.097 - 0.345\\nepoch:13 | 0.097 - 0.345\\nepoch:14 | 0.097 - 0.367\\nepoch:15 | 0.097 - 0.384\\nepoch:16 | 0.097 - 0.387\\nepoch:17 | 0.097 - 0.407\\nepoch:18 | 0.097 - 0.409\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.097 - 0.421\\n============== 2/16 ==============\\nepoch:0 | 0.094 - 0.105\\n/Users/csg/HomeWorkspace/git/DL-scratch1/common/layers.py:12: RuntimeWarning: invalid value encountered in less_equal\\n  self.mask = (x <= 0)\\n/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: overflow encountered in square\\n  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\\n/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: invalid value encountered in double_scalars\\n  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\\nepoch:1 | 0.097 - 0.088\\nepoch:2 | 0.097 - 0.095\\nepoch:3 | 0.097 - 0.118\\nepoch:4 | 0.097 - 0.142\\nepoch:5 | 0.097 - 0.151\\nepoch:6 | 0.097 - 0.192\\nepoch:7 | 0.097 - 0.21\\nepoch:8 | 0.097 - 0.228\\nepoch:9 | 0.097 - 0.251\\nepoch:10 | 0.097 - 0.271\\nepoch:11 | 0.097 - 0.3\\nepoch:12 | 0.097 - 0.322\\nepoch:13 | 0.097 - 0.348\\nepoch:14 | 0.097 - 0.361\\nepoch:15 | 0.097 - 0.392\\nepoch:16 | 0.097 - 0.401\\nepoch:17 | 0.097 - 0.422\\nepoch:18 | 0.097 - 0.442\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.097 - 0.459\\n============== 3/16 ==============\\nepoch:0 | 0.146 - 0.078\\nepoch:1 | 0.305 - 0.103\\nepoch:2 | 0.461 - 0.149\\nepoch:3 | 0.558 - 0.173\\nepoch:4 | 0.633 - 0.193\\nepoch:5 | 0.673 - 0.231\\nepoch:6 | 0.743 - 0.258\\nepoch:7 | 0.777 - 0.298\\nepoch:8 | 0.822 - 0.332\\nepoch:9 | 0.848 - 0.364\\nepoch:10 | 0.866 - 0.399\\nepoch:11 | 0.885 - 0.44\\nepoch:12 | 0.902 - 0.471\\nepoch:13 | 0.919 - 0.507\\nepoch:14 | 0.927 - 0.525\\nepoch:15 | 0.934 - 0.543\\nepoch:16 | 0.939 - 0.57\\nepoch:17 | 0.953 - 0.586\\nepoch:18 | 0.961 - 0.609\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.966 - 0.626\\n============== 4/16 ==============\\nepoch:0 | 0.098 - 0.117\\nepoch:1 | 0.226 - 0.135\\nepoch:2 | 0.378 - 0.183\\nepoch:3 | 0.469 - 0.248\\nepoch:4 | 0.545 - 0.31\\nepoch:5 | 0.607 - 0.369\\nepoch:6 | 0.64 - 0.439\\nepoch:7 | 0.664 - 0.481\\nepoch:8 | 0.713 - 0.519\\nepoch:9 | 0.729 - 0.571\\nepoch:10 | 0.761 - 0.59\\nepoch:11 | 0.766 - 0.63\\nepoch:12 | 0.781 - 0.655\\nepoch:13 | 0.802 - 0.678\\nepoch:14 | 0.809 - 0.698\\nepoch:15 | 0.823 - 0.72\\nepoch:16 | 0.84 - 0.734\\nepoch:17 | 0.839 - 0.757\\nepoch:18 | 0.852 - 0.773\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.858 - 0.78\\n============== 5/16 ==============\\nepoch:0 | 0.115 - 0.102\\nepoch:1 | 0.126 - 0.143\\nepoch:2 | 0.13 - 0.266\\nepoch:3 | 0.143 - 0.411\\nepoch:4 | 0.159 - 0.5\\nepoch:5 | 0.174 - 0.579\\nepoch:6 | 0.183 - 0.628\\nepoch:7 | 0.186 - 0.681\\nepoch:8 | 0.196 - 0.702\\nepoch:9 | 0.201 - 0.736\\nepoch:10 | 0.206 - 0.766\\nepoch:11 | 0.211 - 0.789\\nepoch:12 | 0.215 - 0.809\\nepoch:13 | 0.221 - 0.826\\nepoch:14 | 0.222 - 0.835\\nepoch:15 | 0.237 - 0.844\\nepoch:16 | 0.236 - 0.861\\nepoch:17 | 0.238 - 0.872\\nepoch:18 | 0.25 - 0.874\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.247 - 0.881\\n============== 6/16 ==============\\nepoch:0 | 0.093 - 0.128\\nepoch:1 | 0.116 - 0.271\\nepoch:2 | 0.135 - 0.424\\nepoch:3 | 0.122 - 0.545\\nepoch:4 | 0.116 - 0.639\\nepoch:5 | 0.116 - 0.707\\nepoch:6 | 0.116 - 0.762\\nepoch:7 | 0.116 - 0.786\\nepoch:8 | 0.116 - 0.812\\nepoch:9 | 0.116 - 0.84\\nepoch:10 | 0.116 - 0.858\\nepoch:11 | 0.116 - 0.876\\nepoch:12 | 0.116 - 0.888\\nepoch:13 | 0.116 - 0.903\\nepoch:14 | 0.116 - 0.911\\nepoch:15 | 0.116 - 0.919\\nepoch:16 | 0.116 - 0.925\\nepoch:17 | 0.116 - 0.93\\nepoch:18 | 0.116 - 0.937\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.116 - 0.944\\n============== 7/16 ==============\\nepoch:0 | 0.097 - 0.163\\nepoch:1 | 0.117 - 0.271\\nepoch:2 | 0.117 - 0.589\\nepoch:3 | 0.117 - 0.678\\nepoch:4 | 0.117 - 0.734\\nepoch:5 | 0.117 - 0.769\\nepoch:6 | 0.117 - 0.813\\nepoch:7 | 0.117 - 0.841\\nepoch:8 | 0.117 - 0.872\\nepoch:9 | 0.117 - 0.894\\nepoch:10 | 0.117 - 0.917\\nepoch:11 | 0.117 - 0.935\\nepoch:12 | 0.117 - 0.945\\nepoch:13 | 0.117 - 0.957\\nepoch:14 | 0.117 - 0.966\\nepoch:15 | 0.117 - 0.969\\nepoch:16 | 0.117 - 0.975\\nepoch:17 | 0.117 - 0.978\\nepoch:18 | 0.117 - 0.983\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.117 - 0.985\\n============== 8/16 ==============\\nepoch:0 | 0.1 - 0.136\\nepoch:1 | 0.117 - 0.4\\nepoch:2 | 0.116 - 0.66\\nepoch:3 | 0.116 - 0.753\\nepoch:4 | 0.116 - 0.796\\nepoch:5 | 0.116 - 0.843\\nepoch:6 | 0.116 - 0.863\\nepoch:7 | 0.116 - 0.881\\nepoch:8 | 0.116 - 0.907\\nepoch:9 | 0.116 - 0.927\\nepoch:10 | 0.116 - 0.953\\nepoch:11 | 0.116 - 0.97\\nepoch:12 | 0.116 - 0.981\\nepoch:13 | 0.116 - 0.992\\nepoch:14 | 0.116 - 0.992\\nepoch:15 | 0.116 - 0.994\\nepoch:16 | 0.116 - 0.996\\nepoch:17 | 0.116 - 0.996\\nepoch:18 | 0.116 - 0.996\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.116 - 0.997\\n============== 9/16 ==============\\nepoch:0 | 0.099 - 0.135\\nepoch:1 | 0.116 - 0.616\\nepoch:2 | 0.116 - 0.705\\nepoch:3 | 0.116 - 0.771\\nepoch:4 | 0.116 - 0.853\\nepoch:5 | 0.116 - 0.927\\nepoch:6 | 0.116 - 0.956\\nepoch:7 | 0.116 - 0.971\\nepoch:8 | 0.116 - 0.985\\nepoch:9 | 0.116 - 0.988\\nepoch:10 | 0.116 - 0.991\\nepoch:11 | 0.116 - 0.997\\nepoch:12 | 0.116 - 0.976\\nepoch:13 | 0.116 - 0.999\\nepoch:14 | 0.116 - 1.0\\nepoch:15 | 0.116 - 1.0\\nepoch:16 | 0.116 - 1.0\\nepoch:17 | 0.116 - 1.0\\nepoch:18 | 0.116 - 1.0\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.116 - 1.0\\n============== 10/16 ==============\\nepoch:0 | 0.087 - 0.119\\nepoch:1 | 0.117 - 0.537\\nepoch:2 | 0.117 - 0.67\\nepoch:3 | 0.116 - 0.746\\nepoch:4 | 0.116 - 0.762\\nepoch:5 | 0.116 - 0.764\\nepoch:6 | 0.116 - 0.8\\nepoch:7 | 0.116 - 0.801\\nepoch:8 | 0.116 - 0.807\\nepoch:9 | 0.116 - 0.847\\nepoch:10 | 0.116 - 0.865\\nepoch:11 | 0.116 - 0.871\\nepoch:12 | 0.116 - 0.935\\nepoch:13 | 0.116 - 0.939\\nepoch:14 | 0.116 - 0.966\\nepoch:15 | 0.116 - 0.977\\nepoch:16 | 0.117 - 0.987\\nepoch:17 | 0.117 - 0.989\\nepoch:18 | 0.117 - 0.99\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.116 - 0.991\\n============== 11/16 ==============\\nepoch:0 | 0.094 - 0.119\\nepoch:1 | 0.116 - 0.5\\nepoch:2 | 0.117 - 0.675\\nepoch:3 | 0.117 - 0.653\\nepoch:4 | 0.117 - 0.705\\nepoch:5 | 0.117 - 0.762\\nepoch:6 | 0.117 - 0.725\\nepoch:7 | 0.116 - 0.808\\nepoch:8 | 0.116 - 0.871\\nepoch:9 | 0.116 - 0.761\\nepoch:10 | 0.116 - 0.933\\nepoch:11 | 0.116 - 0.915\\nepoch:12 | 0.116 - 0.973\\nepoch:13 | 0.116 - 0.968\\nepoch:14 | 0.116 - 0.981\\nepoch:15 | 0.116 - 0.953\\nepoch:16 | 0.116 - 0.985\\nepoch:17 | 0.116 - 0.988\\nepoch:18 | 0.116 - 0.987\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.116 - 0.995\\n============== 12/16 ==============\\nepoch:0 | 0.1 - 0.109\\nepoch:1 | 0.117 - 0.471\\nepoch:2 | 0.117 - 0.621\\nepoch:3 | 0.117 - 0.669\\nepoch:4 | 0.117 - 0.671\\nepoch:5 | 0.117 - 0.672\\nepoch:6 | 0.117 - 0.753\\nepoch:7 | 0.117 - 0.754\\nepoch:8 | 0.117 - 0.737\\nepoch:9 | 0.117 - 0.767\\nepoch:10 | 0.117 - 0.739\\nepoch:11 | 0.117 - 0.787\\nepoch:12 | 0.117 - 0.788\\nepoch:13 | 0.117 - 0.798\\nepoch:14 | 0.117 - 0.845\\nepoch:15 | 0.117 - 0.866\\nepoch:16 | 0.117 - 0.911\\nepoch:17 | 0.117 - 0.965\\nepoch:18 | 0.117 - 0.969\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.117 - 0.986\\n============== 13/16 ==============\\nepoch:0 | 0.1 - 0.197\\nepoch:1 | 0.117 - 0.486\\nepoch:2 | 0.117 - 0.519\\nepoch:3 | 0.116 - 0.574\\nepoch:4 | 0.116 - 0.577\\nepoch:5 | 0.116 - 0.653\\nepoch:6 | 0.117 - 0.68\\nepoch:7 | 0.116 - 0.7\\nepoch:8 | 0.117 - 0.678\\nepoch:9 | 0.117 - 0.703\\nepoch:10 | 0.117 - 0.744\\nepoch:11 | 0.117 - 0.804\\nepoch:12 | 0.117 - 0.799\\nepoch:13 | 0.117 - 0.737\\nepoch:14 | 0.117 - 0.807\\nepoch:15 | 0.117 - 0.805\\nepoch:16 | 0.117 - 0.806\\nepoch:17 | 0.117 - 0.812\\nepoch:18 | 0.117 - 0.813\\nNo handles with labels found to put in legend.\\nepoch:19 | 0.117 - 0.814\\n============== 14/16 ==============\\nepoch:0 | 0.1 - 0.1\\nepoch:1 | 0.116 - 0.261\\nepoch:2 | 0.116 - 0.464\\nepoch:3 | 0.116 - 0.38\\nepoch:4 | 0.116 - 0.479\\nepoch:5 | 0.116 - 0.489\\nepoch:6 | 0.117 - 0.474\\nepoch:7 | 0.116 - 0.472\\nepoch:8 | 0.116 - 0.503\\nepoch:9 | 0.116 - 0.491\\nepoch:10 | 0.116 - 0.504\\nepoch:11 | 0.116 - 0.471\\nepoch:12 | 0.116 - 0.516\\nepoch:13 | 0.116 - 0.474\\nepoch:14 | 0.116 - 0.453\\nepoch:15 | 0.116 - 0.484\\nepoch:16 | 0.116 - 0.481\\nepoch:17 | 0.116 - 0.437\\nNo handles with labels found to put in legend.\\nepoch:18 | 0.117 - 0.443\\nepoch:19 | 0.117 - 0.411\\n============== 15/16 ==============\\nepoch:0 | 0.116 - 0.122\\nepoch:1 | 0.116 - 0.301\\nepoch:2 | 0.116 - 0.407\\nepoch:3 | 0.117 - 0.409\\nepoch:4 | 0.117 - 0.413\\nepoch:5 | 0.117 - 0.495\\nepoch:6 | 0.117 - 0.481\\nepoch:7 | 0.117 - 0.507\\nepoch:8 | 0.117 - 0.513\\nepoch:9 | 0.117 - 0.519\\nepoch:10 | 0.117 - 0.509\\nepoch:11 | 0.117 - 0.511\\nepoch:12 | 0.117 - 0.52\\nepoch:13 | 0.117 - 0.519\\nepoch:14 | 0.117 - 0.493\\nepoch:15 | 0.117 - 0.492\\nepoch:16 | 0.117 - 0.506\\nepoch:17 | 0.117 - 0.507\\nepoch:18 | 0.117 - 0.499\\nepoch:19 | 0.117 - 0.51\\nNo handles with labels found to put in legend.\\n============== 16/16 ==============\\nepoch:0 | 0.092 - 0.156\\nepoch:1 | 0.117 - 0.233\\nepoch:2 | 0.116 - 0.378\\nepoch:3 | 0.117 - 0.41\\nepoch:4 | 0.117 - 0.438\\nepoch:5 | 0.117 - 0.426\\nepoch:6 | 0.117 - 0.43\\nepoch:7 | 0.117 - 0.421\\nepoch:8 | 0.117 - 0.43\\nepoch:9 | 0.117 - 0.431\\nepoch:10 | 0.117 - 0.395\\nepoch:11 | 0.117 - 0.421\\nepoch:12 | 0.117 - 0.53\\nepoch:13 | 0.117 - 0.513\\nepoch:14 | 0.117 - 0.507\\nepoch:15 | 0.117 - 0.508\\nepoch:16 | 0.117 - 0.529\\nepoch:17 | 0.117 - 0.533\\nepoch:18 | 0.117 - 0.614\\nepoch:19 | 0.117 - 0.621\\n<Figure size 640x480 with 16 Axes>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "============== 1/16 ==============\n",
    "epoch:0 | 0.093 - 0.094\n",
    "/Users/csg/HomeWorkspace/git/DL-scratch1/common/layers.py:12: RuntimeWarning: invalid value encountered in less_equal\n",
    "  self.mask = (x <= 0)\n",
    "/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: overflow encountered in square\n",
    "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
    "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "epoch:1 | 0.097 - 0.107\n",
    "epoch:2 | 0.097 - 0.137\n",
    "epoch:3 | 0.097 - 0.165\n",
    "epoch:4 | 0.097 - 0.183\n",
    "epoch:5 | 0.097 - 0.205\n",
    "epoch:6 | 0.097 - 0.227\n",
    "epoch:7 | 0.097 - 0.247\n",
    "epoch:8 | 0.097 - 0.27\n",
    "epoch:9 | 0.097 - 0.287\n",
    "epoch:10 | 0.097 - 0.309\n",
    "epoch:11 | 0.097 - 0.324\n",
    "epoch:12 | 0.097 - 0.345\n",
    "epoch:13 | 0.097 - 0.345\n",
    "epoch:14 | 0.097 - 0.367\n",
    "epoch:15 | 0.097 - 0.384\n",
    "epoch:16 | 0.097 - 0.387\n",
    "epoch:17 | 0.097 - 0.407\n",
    "epoch:18 | 0.097 - 0.409\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.097 - 0.421\n",
    "============== 2/16 ==============\n",
    "epoch:0 | 0.094 - 0.105\n",
    "/Users/csg/HomeWorkspace/git/DL-scratch1/common/layers.py:12: RuntimeWarning: invalid value encountered in less_equal\n",
    "  self.mask = (x <= 0)\n",
    "/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: overflow encountered in square\n",
    "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "/Users/csg/HomeWorkspace/git/DL-scratch1/common/multi_layer_net_extend.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
    "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "epoch:1 | 0.097 - 0.088\n",
    "epoch:2 | 0.097 - 0.095\n",
    "epoch:3 | 0.097 - 0.118\n",
    "epoch:4 | 0.097 - 0.142\n",
    "epoch:5 | 0.097 - 0.151\n",
    "epoch:6 | 0.097 - 0.192\n",
    "epoch:7 | 0.097 - 0.21\n",
    "epoch:8 | 0.097 - 0.228\n",
    "epoch:9 | 0.097 - 0.251\n",
    "epoch:10 | 0.097 - 0.271\n",
    "epoch:11 | 0.097 - 0.3\n",
    "epoch:12 | 0.097 - 0.322\n",
    "epoch:13 | 0.097 - 0.348\n",
    "epoch:14 | 0.097 - 0.361\n",
    "epoch:15 | 0.097 - 0.392\n",
    "epoch:16 | 0.097 - 0.401\n",
    "epoch:17 | 0.097 - 0.422\n",
    "epoch:18 | 0.097 - 0.442\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.097 - 0.459\n",
    "============== 3/16 ==============\n",
    "epoch:0 | 0.146 - 0.078\n",
    "epoch:1 | 0.305 - 0.103\n",
    "epoch:2 | 0.461 - 0.149\n",
    "epoch:3 | 0.558 - 0.173\n",
    "epoch:4 | 0.633 - 0.193\n",
    "epoch:5 | 0.673 - 0.231\n",
    "epoch:6 | 0.743 - 0.258\n",
    "epoch:7 | 0.777 - 0.298\n",
    "epoch:8 | 0.822 - 0.332\n",
    "epoch:9 | 0.848 - 0.364\n",
    "epoch:10 | 0.866 - 0.399\n",
    "epoch:11 | 0.885 - 0.44\n",
    "epoch:12 | 0.902 - 0.471\n",
    "epoch:13 | 0.919 - 0.507\n",
    "epoch:14 | 0.927 - 0.525\n",
    "epoch:15 | 0.934 - 0.543\n",
    "epoch:16 | 0.939 - 0.57\n",
    "epoch:17 | 0.953 - 0.586\n",
    "epoch:18 | 0.961 - 0.609\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.966 - 0.626\n",
    "============== 4/16 ==============\n",
    "epoch:0 | 0.098 - 0.117\n",
    "epoch:1 | 0.226 - 0.135\n",
    "epoch:2 | 0.378 - 0.183\n",
    "epoch:3 | 0.469 - 0.248\n",
    "epoch:4 | 0.545 - 0.31\n",
    "epoch:5 | 0.607 - 0.369\n",
    "epoch:6 | 0.64 - 0.439\n",
    "epoch:7 | 0.664 - 0.481\n",
    "epoch:8 | 0.713 - 0.519\n",
    "epoch:9 | 0.729 - 0.571\n",
    "epoch:10 | 0.761 - 0.59\n",
    "epoch:11 | 0.766 - 0.63\n",
    "epoch:12 | 0.781 - 0.655\n",
    "epoch:13 | 0.802 - 0.678\n",
    "epoch:14 | 0.809 - 0.698\n",
    "epoch:15 | 0.823 - 0.72\n",
    "epoch:16 | 0.84 - 0.734\n",
    "epoch:17 | 0.839 - 0.757\n",
    "epoch:18 | 0.852 - 0.773\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.858 - 0.78\n",
    "============== 5/16 ==============\n",
    "epoch:0 | 0.115 - 0.102\n",
    "epoch:1 | 0.126 - 0.143\n",
    "epoch:2 | 0.13 - 0.266\n",
    "epoch:3 | 0.143 - 0.411\n",
    "epoch:4 | 0.159 - 0.5\n",
    "epoch:5 | 0.174 - 0.579\n",
    "epoch:6 | 0.183 - 0.628\n",
    "epoch:7 | 0.186 - 0.681\n",
    "epoch:8 | 0.196 - 0.702\n",
    "epoch:9 | 0.201 - 0.736\n",
    "epoch:10 | 0.206 - 0.766\n",
    "epoch:11 | 0.211 - 0.789\n",
    "epoch:12 | 0.215 - 0.809\n",
    "epoch:13 | 0.221 - 0.826\n",
    "epoch:14 | 0.222 - 0.835\n",
    "epoch:15 | 0.237 - 0.844\n",
    "epoch:16 | 0.236 - 0.861\n",
    "epoch:17 | 0.238 - 0.872\n",
    "epoch:18 | 0.25 - 0.874\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.247 - 0.881\n",
    "============== 6/16 ==============\n",
    "epoch:0 | 0.093 - 0.128\n",
    "epoch:1 | 0.116 - 0.271\n",
    "epoch:2 | 0.135 - 0.424\n",
    "epoch:3 | 0.122 - 0.545\n",
    "epoch:4 | 0.116 - 0.639\n",
    "epoch:5 | 0.116 - 0.707\n",
    "epoch:6 | 0.116 - 0.762\n",
    "epoch:7 | 0.116 - 0.786\n",
    "epoch:8 | 0.116 - 0.812\n",
    "epoch:9 | 0.116 - 0.84\n",
    "epoch:10 | 0.116 - 0.858\n",
    "epoch:11 | 0.116 - 0.876\n",
    "epoch:12 | 0.116 - 0.888\n",
    "epoch:13 | 0.116 - 0.903\n",
    "epoch:14 | 0.116 - 0.911\n",
    "epoch:15 | 0.116 - 0.919\n",
    "epoch:16 | 0.116 - 0.925\n",
    "epoch:17 | 0.116 - 0.93\n",
    "epoch:18 | 0.116 - 0.937\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.116 - 0.944\n",
    "============== 7/16 ==============\n",
    "epoch:0 | 0.097 - 0.163\n",
    "epoch:1 | 0.117 - 0.271\n",
    "epoch:2 | 0.117 - 0.589\n",
    "epoch:3 | 0.117 - 0.678\n",
    "epoch:4 | 0.117 - 0.734\n",
    "epoch:5 | 0.117 - 0.769\n",
    "epoch:6 | 0.117 - 0.813\n",
    "epoch:7 | 0.117 - 0.841\n",
    "epoch:8 | 0.117 - 0.872\n",
    "epoch:9 | 0.117 - 0.894\n",
    "epoch:10 | 0.117 - 0.917\n",
    "epoch:11 | 0.117 - 0.935\n",
    "epoch:12 | 0.117 - 0.945\n",
    "epoch:13 | 0.117 - 0.957\n",
    "epoch:14 | 0.117 - 0.966\n",
    "epoch:15 | 0.117 - 0.969\n",
    "epoch:16 | 0.117 - 0.975\n",
    "epoch:17 | 0.117 - 0.978\n",
    "epoch:18 | 0.117 - 0.983\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.117 - 0.985\n",
    "============== 8/16 ==============\n",
    "epoch:0 | 0.1 - 0.136\n",
    "epoch:1 | 0.117 - 0.4\n",
    "epoch:2 | 0.116 - 0.66\n",
    "epoch:3 | 0.116 - 0.753\n",
    "epoch:4 | 0.116 - 0.796\n",
    "epoch:5 | 0.116 - 0.843\n",
    "epoch:6 | 0.116 - 0.863\n",
    "epoch:7 | 0.116 - 0.881\n",
    "epoch:8 | 0.116 - 0.907\n",
    "epoch:9 | 0.116 - 0.927\n",
    "epoch:10 | 0.116 - 0.953\n",
    "epoch:11 | 0.116 - 0.97\n",
    "epoch:12 | 0.116 - 0.981\n",
    "epoch:13 | 0.116 - 0.992\n",
    "epoch:14 | 0.116 - 0.992\n",
    "epoch:15 | 0.116 - 0.994\n",
    "epoch:16 | 0.116 - 0.996\n",
    "epoch:17 | 0.116 - 0.996\n",
    "epoch:18 | 0.116 - 0.996\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.116 - 0.997\n",
    "============== 9/16 ==============\n",
    "epoch:0 | 0.099 - 0.135\n",
    "epoch:1 | 0.116 - 0.616\n",
    "epoch:2 | 0.116 - 0.705\n",
    "epoch:3 | 0.116 - 0.771\n",
    "epoch:4 | 0.116 - 0.853\n",
    "epoch:5 | 0.116 - 0.927\n",
    "epoch:6 | 0.116 - 0.956\n",
    "epoch:7 | 0.116 - 0.971\n",
    "epoch:8 | 0.116 - 0.985\n",
    "epoch:9 | 0.116 - 0.988\n",
    "epoch:10 | 0.116 - 0.991\n",
    "epoch:11 | 0.116 - 0.997\n",
    "epoch:12 | 0.116 - 0.976\n",
    "epoch:13 | 0.116 - 0.999\n",
    "epoch:14 | 0.116 - 1.0\n",
    "epoch:15 | 0.116 - 1.0\n",
    "epoch:16 | 0.116 - 1.0\n",
    "epoch:17 | 0.116 - 1.0\n",
    "epoch:18 | 0.116 - 1.0\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.116 - 1.0\n",
    "============== 10/16 ==============\n",
    "epoch:0 | 0.087 - 0.119\n",
    "epoch:1 | 0.117 - 0.537\n",
    "epoch:2 | 0.117 - 0.67\n",
    "epoch:3 | 0.116 - 0.746\n",
    "epoch:4 | 0.116 - 0.762\n",
    "epoch:5 | 0.116 - 0.764\n",
    "epoch:6 | 0.116 - 0.8\n",
    "epoch:7 | 0.116 - 0.801\n",
    "epoch:8 | 0.116 - 0.807\n",
    "epoch:9 | 0.116 - 0.847\n",
    "epoch:10 | 0.116 - 0.865\n",
    "epoch:11 | 0.116 - 0.871\n",
    "epoch:12 | 0.116 - 0.935\n",
    "epoch:13 | 0.116 - 0.939\n",
    "epoch:14 | 0.116 - 0.966\n",
    "epoch:15 | 0.116 - 0.977\n",
    "epoch:16 | 0.117 - 0.987\n",
    "epoch:17 | 0.117 - 0.989\n",
    "epoch:18 | 0.117 - 0.99\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.116 - 0.991\n",
    "============== 11/16 ==============\n",
    "epoch:0 | 0.094 - 0.119\n",
    "epoch:1 | 0.116 - 0.5\n",
    "epoch:2 | 0.117 - 0.675\n",
    "epoch:3 | 0.117 - 0.653\n",
    "epoch:4 | 0.117 - 0.705\n",
    "epoch:5 | 0.117 - 0.762\n",
    "epoch:6 | 0.117 - 0.725\n",
    "epoch:7 | 0.116 - 0.808\n",
    "epoch:8 | 0.116 - 0.871\n",
    "epoch:9 | 0.116 - 0.761\n",
    "epoch:10 | 0.116 - 0.933\n",
    "epoch:11 | 0.116 - 0.915\n",
    "epoch:12 | 0.116 - 0.973\n",
    "epoch:13 | 0.116 - 0.968\n",
    "epoch:14 | 0.116 - 0.981\n",
    "epoch:15 | 0.116 - 0.953\n",
    "epoch:16 | 0.116 - 0.985\n",
    "epoch:17 | 0.116 - 0.988\n",
    "epoch:18 | 0.116 - 0.987\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.116 - 0.995\n",
    "============== 12/16 ==============\n",
    "epoch:0 | 0.1 - 0.109\n",
    "epoch:1 | 0.117 - 0.471\n",
    "epoch:2 | 0.117 - 0.621\n",
    "epoch:3 | 0.117 - 0.669\n",
    "epoch:4 | 0.117 - 0.671\n",
    "epoch:5 | 0.117 - 0.672\n",
    "epoch:6 | 0.117 - 0.753\n",
    "epoch:7 | 0.117 - 0.754\n",
    "epoch:8 | 0.117 - 0.737\n",
    "epoch:9 | 0.117 - 0.767\n",
    "epoch:10 | 0.117 - 0.739\n",
    "epoch:11 | 0.117 - 0.787\n",
    "epoch:12 | 0.117 - 0.788\n",
    "epoch:13 | 0.117 - 0.798\n",
    "epoch:14 | 0.117 - 0.845\n",
    "epoch:15 | 0.117 - 0.866\n",
    "epoch:16 | 0.117 - 0.911\n",
    "epoch:17 | 0.117 - 0.965\n",
    "epoch:18 | 0.117 - 0.969\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.117 - 0.986\n",
    "============== 13/16 ==============\n",
    "epoch:0 | 0.1 - 0.197\n",
    "epoch:1 | 0.117 - 0.486\n",
    "epoch:2 | 0.117 - 0.519\n",
    "epoch:3 | 0.116 - 0.574\n",
    "epoch:4 | 0.116 - 0.577\n",
    "epoch:5 | 0.116 - 0.653\n",
    "epoch:6 | 0.117 - 0.68\n",
    "epoch:7 | 0.116 - 0.7\n",
    "epoch:8 | 0.117 - 0.678\n",
    "epoch:9 | 0.117 - 0.703\n",
    "epoch:10 | 0.117 - 0.744\n",
    "epoch:11 | 0.117 - 0.804\n",
    "epoch:12 | 0.117 - 0.799\n",
    "epoch:13 | 0.117 - 0.737\n",
    "epoch:14 | 0.117 - 0.807\n",
    "epoch:15 | 0.117 - 0.805\n",
    "epoch:16 | 0.117 - 0.806\n",
    "epoch:17 | 0.117 - 0.812\n",
    "epoch:18 | 0.117 - 0.813\n",
    "No handles with labels found to put in legend.\n",
    "epoch:19 | 0.117 - 0.814\n",
    "============== 14/16 ==============\n",
    "epoch:0 | 0.1 - 0.1\n",
    "epoch:1 | 0.116 - 0.261\n",
    "epoch:2 | 0.116 - 0.464\n",
    "epoch:3 | 0.116 - 0.38\n",
    "epoch:4 | 0.116 - 0.479\n",
    "epoch:5 | 0.116 - 0.489\n",
    "epoch:6 | 0.117 - 0.474\n",
    "epoch:7 | 0.116 - 0.472\n",
    "epoch:8 | 0.116 - 0.503\n",
    "epoch:9 | 0.116 - 0.491\n",
    "epoch:10 | 0.116 - 0.504\n",
    "epoch:11 | 0.116 - 0.471\n",
    "epoch:12 | 0.116 - 0.516\n",
    "epoch:13 | 0.116 - 0.474\n",
    "epoch:14 | 0.116 - 0.453\n",
    "epoch:15 | 0.116 - 0.484\n",
    "epoch:16 | 0.116 - 0.481\n",
    "epoch:17 | 0.116 - 0.437\n",
    "No handles with labels found to put in legend.\n",
    "epoch:18 | 0.117 - 0.443\n",
    "epoch:19 | 0.117 - 0.411\n",
    "============== 15/16 ==============\n",
    "epoch:0 | 0.116 - 0.122\n",
    "epoch:1 | 0.116 - 0.301\n",
    "epoch:2 | 0.116 - 0.407\n",
    "epoch:3 | 0.117 - 0.409\n",
    "epoch:4 | 0.117 - 0.413\n",
    "epoch:5 | 0.117 - 0.495\n",
    "epoch:6 | 0.117 - 0.481\n",
    "epoch:7 | 0.117 - 0.507\n",
    "epoch:8 | 0.117 - 0.513\n",
    "epoch:9 | 0.117 - 0.519\n",
    "epoch:10 | 0.117 - 0.509\n",
    "epoch:11 | 0.117 - 0.511\n",
    "epoch:12 | 0.117 - 0.52\n",
    "epoch:13 | 0.117 - 0.519\n",
    "epoch:14 | 0.117 - 0.493\n",
    "epoch:15 | 0.117 - 0.492\n",
    "epoch:16 | 0.117 - 0.506\n",
    "epoch:17 | 0.117 - 0.507\n",
    "epoch:18 | 0.117 - 0.499\n",
    "epoch:19 | 0.117 - 0.51\n",
    "No handles with labels found to put in legend.\n",
    "============== 16/16 ==============\n",
    "epoch:0 | 0.092 - 0.156\n",
    "epoch:1 | 0.117 - 0.233\n",
    "epoch:2 | 0.116 - 0.378\n",
    "epoch:3 | 0.117 - 0.41\n",
    "epoch:4 | 0.117 - 0.438\n",
    "epoch:5 | 0.117 - 0.426\n",
    "epoch:6 | 0.117 - 0.43\n",
    "epoch:7 | 0.117 - 0.421\n",
    "epoch:8 | 0.117 - 0.43\n",
    "epoch:9 | 0.117 - 0.431\n",
    "epoch:10 | 0.117 - 0.395\n",
    "epoch:11 | 0.117 - 0.421\n",
    "epoch:12 | 0.117 - 0.53\n",
    "epoch:13 | 0.117 - 0.513\n",
    "epoch:14 | 0.117 - 0.507\n",
    "epoch:15 | 0.117 - 0.508\n",
    "epoch:16 | 0.117 - 0.529\n",
    "epoch:17 | 0.117 - 0.533\n",
    "epoch:18 | 0.117 - 0.614\n",
    "epoch:19 | 0.117 - 0.621\n",
    "<Figure size 640x480 with 16 Axes>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter_optimization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:0.23 | lr:0.00041573680063862624, weight decay:1.7421173695009184e-08\n",
      "val acc:0.73 | lr:0.008059345570336525, weight decay:5.38942229384971e-07\n",
      "val acc:0.11 | lr:8.086476915678654e-06, weight decay:2.388842811212531e-06\n",
      "val acc:0.06 | lr:1.5004191392841905e-05, weight decay:1.478624936323425e-08\n",
      "val acc:0.82 | lr:0.008810757614763862, weight decay:2.4710151564284837e-05\n",
      "val acc:0.29 | lr:0.0020181433279115746, weight decay:2.215813011399626e-08\n",
      "val acc:0.67 | lr:0.00544059305428972, weight decay:5.410682047313105e-06\n",
      "val acc:0.09 | lr:8.564567123043342e-05, weight decay:3.709477327421879e-06\n",
      "val acc:0.79 | lr:0.008759308709890798, weight decay:1.774108350135052e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-561729f03e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# ================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mval_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val acc:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" | lr:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", weight decay:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lr:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", weight decay:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-561729f03e76>\u001b[0m in \u001b[0;36m__train\u001b[0;34m(lr, weight_decay, epocs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                       optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HomeWorkspace/git/DL-scratch1/common/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HomeWorkspace/git/DL-scratch1/common/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mx_test_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HomeWorkspace/git/DL-scratch1/common/multi_layer_net.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HomeWorkspace/git/DL-scratch1/common/multi_layer_net.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HomeWorkspace/git/DL-scratch1/common/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.util import shuffle_dataset\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 결과를 빠르게 얻기 위해 훈련 데이터를 줄임\n",
    "x_train = x_train[:500]\n",
    "t_train = t_train[:500]\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]\n",
    "\n",
    "\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 무작위 탐색======================================\n",
    "optimization_trial = 100\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "for _ in range(optimization_trial):\n",
    "    # 탐색한 하이퍼파라미터의 범위 지정===============\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "    # ================================================\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list\n",
    "\n",
    "# 그래프 그리기========================================================\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer_compare_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import *\n",
    "\n",
    "\n",
    "# 0. MNIST 데이터 읽기==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "\n",
    "# 1. 실험용 설정==========\n",
    "optimizers = {}\n",
    "optimizers['SGD'] = SGD()\n",
    "optimizers['Momentum'] = Momentum()\n",
    "optimizers['AdaGrad'] = AdaGrad()\n",
    "optimizers['Adam'] = Adam()\n",
    "#optimizers['RMSprop'] = RMSprop()\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key in optimizers.keys():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "        output_size=10)\n",
    "    train_loss[key] = []    \n",
    "\n",
    "\n",
    "# 2. 훈련 시작==========\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in optimizers.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizers[key].update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in optimizers.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "\n",
    "# 3. 그래프 그리기==========\n",
    "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
    "x = np.arange(max_iterations)\n",
    "for key in optimizers.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer_compare_naive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from common.optimizer import *\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 / 20.0 + y**2\n",
    "\n",
    "\n",
    "def df(x, y):\n",
    "    return x / 10.0, 2.0*y\n",
    "\n",
    "init_pos = (-7.0, 2.0)\n",
    "params = {}\n",
    "params['x'], params['y'] = init_pos[0], init_pos[1]\n",
    "grads = {}\n",
    "grads['x'], grads['y'] = 0, 0\n",
    "\n",
    "\n",
    "optimizers = OrderedDict()\n",
    "optimizers[\"SGD\"] = SGD(lr=0.95)\n",
    "optimizers[\"Momentum\"] = Momentum(lr=0.1)\n",
    "optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)\n",
    "optimizers[\"Adam\"] = Adam(lr=0.3)\n",
    "\n",
    "idx = 1\n",
    "\n",
    "for key in optimizers:\n",
    "    optimizer = optimizers[key]\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    params['x'], params['y'] = init_pos[0], init_pos[1]\n",
    "    \n",
    "    for i in range(30):\n",
    "        x_history.append(params['x'])\n",
    "        y_history.append(params['y'])\n",
    "        \n",
    "        grads['x'], grads['y'] = df(params['x'], params['y'])\n",
    "        optimizer.update(params, grads)\n",
    "    \n",
    "\n",
    "    x = np.arange(-10, 10, 0.01)\n",
    "    y = np.arange(-5, 5, 0.01)\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y) \n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # 외곽선 단순화\n",
    "    mask = Z > 7\n",
    "    Z[mask] = 0\n",
    "    \n",
    "    # 그래프 그리기\n",
    "    plt.subplot(2, 2, idx)\n",
    "    idx += 1\n",
    "    plt.plot(x_history, y_history, 'o-', color=\"red\")\n",
    "    plt.contour(X, Y, Z)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.plot(0, 0, '+')\n",
    "    #colorbar()\n",
    "    #spring()\n",
    "    plt.title(key)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overfit_dropout.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overfit_weight_decay.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（가중치 감쇠） 설정 =======================\n",
    "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight_init_activation_histogram.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
    "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
    "hidden_layer_size = 5  # 은닉층이 5개\n",
    "activations = {}  # 이곳에 활성화 결과를 저장\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
    "    w = np.random.randn(node_num, node_num) * 1\n",
    "    # w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 활성화 함수도 바꿔가며 실험해보자！\n",
    "    z = sigmoid(a)\n",
    "    # z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# 히스토그램 그리기\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_init_compare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "\n",
    "# 0. MNIST 데이터 읽기==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "\n",
    "# 1. 실험용 설정==========\n",
    "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key, weight_type in weight_init_types.items():\n",
    "    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "                                  output_size=10, weight_init_std=weight_type)\n",
    "    train_loss[key] = []\n",
    "\n",
    "\n",
    "# 2. 훈련 시작==========\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in weight_init_types.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizer.update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in weight_init_types.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "\n",
    "# 3. 그래프 그리기==========\n",
    "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
    "x = np.arange(max_iterations)\n",
    "for key in weight_init_types.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
