{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 설명\n",
    "| 파일명 | 파일 용도 | 관련 절 | 페이지 |\n",
    "|:--   |:--      |:--    |:--      |\n",
    "| apply_filter.py | lena_gray.png 파일에 필터를 적용합니다. |  |  |\n",
    "| gradient_check.py | SimpleCovNet이 기울기를 올바로 계산하는지 확인합니다. |  |  |\n",
    "| params.pkl | 미리 학습된 가중치 값들입니다. |  |  |\n",
    "| simple_convnet.py | “Convolution-ReLU-Pooling-Affine-ReLU-Affine-Softmax” 순으로 흐르는 단순한 합성곱 신경망(CNN)입니다. | 7.5 CNN 구현하기 | 251 |\n",
    "| train_convnet.py | SimpleConvNet으로 MNIST 데이터셋을 학습합니다. | 7.5 CNN 구현하기 | 254 |\n",
    "| visualize_filter.py | 합성곱 1번째 층의 가중치를 학습 전과 후로 나눠 시각화해봅니다. 이미 학습된 가중치 값(params.pkl)을 읽어서 사용하므로 학습 과정은 생략됩니다. | 7.6.1 1번째 층의 가중치 시각화하기 | 254 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple_convnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_convnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3000693556496277\n",
      "=== epoch:1, train acc:0.16, test acc:0.165 ===\n",
      "train loss:2.29738863699952\n",
      "train loss:2.2930325960147107\n",
      "train loss:2.2877950059157572\n",
      "train loss:2.2779130999270243\n",
      "train loss:2.2689089924777157\n",
      "train loss:2.2541971481286582\n",
      "train loss:2.2443566054434414\n",
      "train loss:2.18753638211375\n",
      "train loss:2.178349502409113\n",
      "train loss:2.1633546425521177\n",
      "train loss:2.110792756961793\n",
      "train loss:2.1251868588589686\n",
      "train loss:1.9732822773563676\n",
      "train loss:1.9836823887687407\n",
      "train loss:1.9334944178241478\n",
      "train loss:1.8900552825711572\n",
      "train loss:1.7972655501129653\n",
      "train loss:1.7189154149304584\n",
      "train loss:1.6562984281776267\n",
      "train loss:1.5565783034060843\n",
      "train loss:1.5106824965231425\n",
      "train loss:1.4390595467605813\n",
      "train loss:1.2958846099521137\n",
      "train loss:1.305880509886082\n",
      "train loss:1.2385622429347511\n",
      "train loss:1.1063357520187649\n",
      "train loss:1.1007814612489515\n",
      "train loss:1.1266010205407566\n",
      "train loss:0.9626975226587059\n",
      "train loss:0.9756425789964556\n",
      "train loss:0.8868166073888595\n",
      "train loss:0.7269942981433016\n",
      "train loss:0.7138784663013311\n",
      "train loss:0.7575687241748323\n",
      "train loss:0.8084349690686058\n",
      "train loss:0.8145907624676514\n",
      "train loss:0.6753315820678811\n",
      "train loss:0.6319029458197365\n",
      "train loss:0.6166223725801584\n",
      "train loss:0.7742076600776335\n",
      "train loss:0.6568506332449391\n",
      "train loss:0.750266799557909\n",
      "train loss:0.56575523272873\n",
      "train loss:0.6189244670092322\n",
      "train loss:0.5308801600665695\n",
      "train loss:0.7159634667804322\n",
      "train loss:0.53338510568961\n",
      "train loss:0.4889691680719054\n",
      "train loss:0.5388739395986399\n",
      "train loss:0.40722077243482724\n",
      "train loss:0.5753395665270454\n",
      "train loss:0.37561782190302706\n",
      "train loss:0.47931618582480817\n",
      "train loss:0.6975918026488475\n",
      "train loss:0.5362145609613213\n",
      "train loss:0.6130242817727699\n",
      "train loss:0.5385027548912713\n",
      "train loss:0.35613411090304326\n",
      "train loss:0.5033802330145327\n",
      "train loss:0.45956889159580855\n",
      "train loss:0.4698448879247318\n",
      "train loss:0.615957176389345\n",
      "train loss:0.4665339138687395\n",
      "train loss:0.663570936813493\n",
      "train loss:0.5055103685389967\n",
      "train loss:0.45758571457551306\n",
      "train loss:0.5317682890257033\n",
      "train loss:0.3237915177211223\n",
      "train loss:0.44283636134256876\n",
      "train loss:0.5133388867242171\n",
      "train loss:0.5065501473257935\n",
      "train loss:0.5807586017733545\n",
      "train loss:0.37836553121464506\n",
      "train loss:0.32464585828124337\n",
      "train loss:0.4692280625997312\n",
      "train loss:0.44279834545389407\n",
      "train loss:0.4965757439207149\n",
      "train loss:0.36238420275442496\n",
      "train loss:0.248337855779528\n",
      "train loss:0.4251069902341252\n",
      "train loss:0.42049854526627173\n",
      "train loss:0.4662858934156167\n",
      "train loss:0.38998690806771386\n",
      "train loss:0.4421234816521404\n",
      "train loss:0.4637986966723169\n",
      "train loss:0.4819519646858142\n",
      "train loss:0.4161021799793064\n",
      "train loss:0.4642249178531669\n",
      "train loss:0.6081408404479688\n",
      "train loss:0.28742923290258665\n",
      "train loss:0.32290201350330977\n",
      "train loss:0.5370822510594021\n",
      "train loss:0.39323917432094985\n",
      "train loss:0.44356739040783016\n",
      "train loss:0.20511562854194787\n",
      "train loss:0.3541335173847884\n",
      "train loss:0.3530409581753846\n",
      "train loss:0.36559738993644764\n",
      "train loss:0.554579121348588\n",
      "train loss:0.33174140284552406\n",
      "train loss:0.30206376270897445\n",
      "train loss:0.42187257192251065\n",
      "train loss:0.45525984104125106\n",
      "train loss:0.4150943527062036\n",
      "train loss:0.2654953061756408\n",
      "train loss:0.48302865067238243\n",
      "train loss:0.505433411561319\n",
      "train loss:0.39370383257597785\n",
      "train loss:0.2964885880079411\n",
      "train loss:0.34535973174373213\n",
      "train loss:0.3168760523910991\n",
      "train loss:0.4483745742295808\n",
      "train loss:0.32075290662957956\n",
      "train loss:0.26577213201747396\n",
      "train loss:0.4818159987683453\n",
      "train loss:0.3501613980210294\n",
      "train loss:0.4720992920990083\n",
      "train loss:0.315376351432687\n",
      "train loss:0.43781978669774774\n",
      "train loss:0.3420601471871641\n",
      "train loss:0.3593394852990027\n",
      "train loss:0.29551756628561726\n",
      "train loss:0.25657552467649347\n",
      "train loss:0.24546758039002572\n",
      "train loss:0.3179933782041051\n",
      "train loss:0.32220808594999645\n",
      "train loss:0.26500942144508977\n",
      "train loss:0.4089055361801071\n",
      "train loss:0.4847114014797374\n",
      "train loss:0.5668109602451424\n",
      "train loss:0.2686858097938929\n",
      "train loss:0.22098278653288922\n",
      "train loss:0.3600870974226061\n",
      "train loss:0.3816613901798644\n",
      "train loss:0.25911549610615636\n",
      "train loss:0.27949357026939237\n",
      "train loss:0.1921822638142744\n",
      "train loss:0.3157639139188466\n",
      "train loss:0.39239066643264053\n",
      "train loss:0.26489723212009514\n",
      "train loss:0.42186819645463286\n",
      "train loss:0.22916255877494165\n",
      "train loss:0.2535228601461709\n",
      "train loss:0.20583684563276106\n",
      "train loss:0.26064957721825516\n",
      "train loss:0.26622307566865816\n",
      "train loss:0.23856493499163028\n",
      "train loss:0.42118595131372843\n",
      "train loss:0.5004645993371463\n",
      "train loss:0.3797539385776371\n",
      "train loss:0.31717743571491414\n",
      "train loss:0.19138380717837536\n",
      "train loss:0.27625734682091496\n",
      "train loss:0.4420105334924513\n",
      "train loss:0.33466924334459264\n",
      "train loss:0.28883471854705667\n",
      "train loss:0.38216289652365903\n",
      "train loss:0.32801585541655237\n",
      "train loss:0.3822664920088284\n",
      "train loss:0.24538023638121675\n",
      "train loss:0.29745022037498925\n",
      "train loss:0.29409478204708234\n",
      "train loss:0.5760398082161374\n",
      "train loss:0.14228985638926062\n",
      "train loss:0.22395656957095955\n",
      "train loss:0.4636137822841902\n",
      "train loss:0.3927547938907467\n",
      "train loss:0.23760943565688394\n",
      "train loss:0.3252652416881398\n",
      "train loss:0.2330304582581263\n",
      "train loss:0.2863892379144183\n",
      "train loss:0.36620792868004753\n",
      "train loss:0.42392946110425345\n",
      "train loss:0.2873035446090467\n",
      "train loss:0.3495205764704722\n",
      "train loss:0.2749604575677657\n",
      "train loss:0.17620455350735498\n",
      "train loss:0.362100258983171\n",
      "train loss:0.40445345610709355\n",
      "train loss:0.199554502412327\n",
      "train loss:0.40478426307522963\n",
      "train loss:0.44437642440920605\n",
      "train loss:0.33424569821767824\n",
      "train loss:0.3100177031751102\n",
      "train loss:0.2796880709519509\n",
      "train loss:0.27558049396567613\n",
      "train loss:0.2846954528031612\n",
      "train loss:0.2763206388636911\n",
      "train loss:0.23834336177970453\n",
      "train loss:0.29283885405938864\n",
      "train loss:0.17165500167808057\n",
      "train loss:0.24784470345832\n",
      "train loss:0.15536981373453973\n",
      "train loss:0.2794256693008227\n",
      "train loss:0.16437813878641733\n",
      "train loss:0.4157990419864193\n",
      "train loss:0.15976052099685367\n",
      "train loss:0.35122480462567884\n",
      "train loss:0.3030579492703639\n",
      "train loss:0.24197325372285572\n",
      "train loss:0.48973529483106576\n",
      "train loss:0.314383875517638\n",
      "train loss:0.22989192577979906\n",
      "train loss:0.29758249684476934\n",
      "train loss:0.1618811487140406\n",
      "train loss:0.28895283491449947\n",
      "train loss:0.35760910735810614\n",
      "train loss:0.2894841239257442\n",
      "train loss:0.1505864704430824\n",
      "train loss:0.25062814452081594\n",
      "train loss:0.20969156430901792\n",
      "train loss:0.29795396511804684\n",
      "train loss:0.4234067148701131\n",
      "train loss:0.25018408374135886\n",
      "train loss:0.20391451359173707\n",
      "train loss:0.24156285576900022\n",
      "train loss:0.39371714347702036\n",
      "train loss:0.31684903185927515\n",
      "train loss:0.3363913609738718\n",
      "train loss:0.2117651119327776\n",
      "train loss:0.1637022681228167\n",
      "train loss:0.28755904136406785\n",
      "train loss:0.3351963358749886\n",
      "train loss:0.2626834831903555\n",
      "train loss:0.16673549831262377\n",
      "train loss:0.4112935076774374\n",
      "train loss:0.34507209032059855\n",
      "train loss:0.10089635450551589\n",
      "train loss:0.1890943394229367\n",
      "train loss:0.2535443134217727\n",
      "train loss:0.3017464564095294\n",
      "train loss:0.2333317632578369\n",
      "train loss:0.3069251120364822\n",
      "train loss:0.2408140634941367\n",
      "train loss:0.37079814976097386\n",
      "train loss:0.36103985549982176\n",
      "train loss:0.2549363177219392\n",
      "train loss:0.24519099237744058\n",
      "train loss:0.28348673335558044\n",
      "train loss:0.1979198215654242\n",
      "train loss:0.36410517439446793\n",
      "train loss:0.2941512221070743\n",
      "train loss:0.35277764713923465\n",
      "train loss:0.285637333917523\n",
      "train loss:0.2517411158568778\n",
      "train loss:0.16023517981306498\n",
      "train loss:0.23193933814047407\n",
      "train loss:0.17204322472546593\n",
      "train loss:0.30597339946360225\n",
      "train loss:0.15271835901103906\n",
      "train loss:0.35097936965396925\n",
      "train loss:0.26054289769268724\n",
      "train loss:0.21625895152551236\n",
      "train loss:0.2305623488256267\n",
      "train loss:0.19831205719885783\n",
      "train loss:0.32353134979751247\n",
      "train loss:0.43750102132753743\n",
      "train loss:0.29883481142031726\n",
      "train loss:0.14032596297682126\n",
      "train loss:0.2335001422886029\n",
      "train loss:0.12977748620404095\n",
      "train loss:0.35720889150969287\n",
      "train loss:0.23404685276962053\n",
      "train loss:0.18111101087197234\n",
      "train loss:0.16146284716875792\n",
      "train loss:0.26412728381761175\n",
      "train loss:0.28399907387458173\n",
      "train loss:0.33493240142753644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.17566209656388165\n",
      "train loss:0.1787422856203694\n",
      "train loss:0.3023694480263617\n",
      "train loss:0.16958877049466278\n",
      "train loss:0.18539021771916386\n",
      "train loss:0.19358599010732086\n",
      "train loss:0.2973824483964519\n",
      "train loss:0.26194483676414415\n",
      "train loss:0.15337611438320625\n",
      "train loss:0.30273807989639495\n",
      "train loss:0.2560699602586245\n",
      "train loss:0.27761232521950663\n",
      "train loss:0.23185332227200117\n",
      "train loss:0.23285521060843667\n",
      "train loss:0.17961160897808917\n",
      "train loss:0.21399826134598965\n",
      "train loss:0.16077761081448755\n",
      "train loss:0.22378580765242007\n",
      "train loss:0.23044003664727866\n",
      "train loss:0.17020751023319605\n",
      "train loss:0.27605222180178585\n",
      "train loss:0.20262042779983813\n",
      "train loss:0.2636649428908451\n",
      "train loss:0.14174243649686502\n",
      "train loss:0.27788445656046445\n",
      "train loss:0.29784709665572673\n",
      "train loss:0.20212264806499516\n",
      "train loss:0.2740174319606695\n",
      "train loss:0.193805356687833\n",
      "train loss:0.12367209051695596\n",
      "train loss:0.2692328015200743\n",
      "train loss:0.14814242709819359\n",
      "train loss:0.17770008055916958\n",
      "train loss:0.15654788277212078\n",
      "train loss:0.1496473430116599\n",
      "train loss:0.29416453968799877\n",
      "train loss:0.1578856229837319\n",
      "train loss:0.17798320908159515\n",
      "train loss:0.1262317635907378\n",
      "train loss:0.31506363490855666\n",
      "train loss:0.1645649614858603\n",
      "train loss:0.15437615065763044\n",
      "train loss:0.21672087413544122\n",
      "train loss:0.16176356957028004\n",
      "train loss:0.27029967531922383\n",
      "train loss:0.14477388132182067\n",
      "train loss:0.15411259752890355\n",
      "train loss:0.2056982951133942\n",
      "train loss:0.34888069926190984\n",
      "train loss:0.21459490335300838\n",
      "train loss:0.17078956137629253\n",
      "train loss:0.14977209093781454\n",
      "train loss:0.1466509737357919\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9706000f44a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# 매개변수 보존\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HomeWorkspace/git/DL-scratch1/common/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HomeWorkspace/git/DL-scratch1/common/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train loss:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5897f62165b6>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0m정답\u001b[0m \u001b[0m레이블\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \"\"\"\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5897f62165b6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HomeWorkspace/git/DL-scratch1/common/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize_filter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "#from simple_convnet import SimpleConvNet\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,10, 10), \n",
    "                        conv_param = {'filter_num':10, 'filter_size':3, 'pad':0, 'stride':1},\n",
    "                        hidden_size=10, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "X = np.random.rand(100).reshape((1, 1, 10, 10))\n",
    "T = np.array([1]).reshape((1,1))\n",
    "\n",
    "grad_num = network.numerical_gradient(X, T)\n",
    "grad = network.gradient(X, T)\n",
    "\n",
    "for key, val in grad_num.items():\n",
    "    print(key, np.abs(grad_num[key] - grad[key]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply_filter.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simple_convnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3cdae9ca7118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimple_convnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleConvNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simple_convnet'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from simple_convnet import SimpleConvNet\n",
    "from matplotlib.image import imread\n",
    "from common.layers import Convolution\n",
    "\n",
    "def filter_show(filters, nx=4, show_num=16):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(show_num / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(show_num):\n",
    "        ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "\n",
    "filter_show(network.params['W1'], 16)\n",
    "\n",
    "img = imread('../dataset/lena_gray.png')\n",
    "img = img.reshape(1, 1, *img.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "w_idx = 1\n",
    "\n",
    "for i in range(16):\n",
    "    w = network.params['W1'][i]\n",
    "    b = 0  # network.params['b1'][i]\n",
    "\n",
    "    w = w.reshape(1, *w.shape)\n",
    "    #b = b.reshape(1, *b.shape)\n",
    "    conv_layer = Convolution(w, b) \n",
    "    out = conv_layer.forward(img)\n",
    "    out = out.reshape(out.shape[2], out.shape[3])\n",
    "    \n",
    "    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(out, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
