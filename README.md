# DL-scratch1

## 1장 헬로 파이썬

> **목차**

```
1.1 파이썬이란?
1.2 파이썬 설치하기
__1.2.1 파이썬 버전
__1.2.2 사용하는 외부 라이브러리
__1.2.3 아나콘다 배포판
1.3 파이썬 인터프리터 
__1.3.1 산술 연산 
__1.3.2 자료형 
__1.3.3 변수 
__1.3.4 리스트 
__1.3.5 딕셔너리
__1.3.6 bool 
__1.3.7 if 문 
__1.3.8 for 문 
__1.3.9 함수 
1.4 파이썬 스크립트 파일 
__1.4.1 파일로 저장하기 
__1.4.2 클래스 
1.5 넘파이 
__1.5.1 넘파이 가져오기 
__1.5.2 넘파이 배열 생성하기 
__1.5.3 넘파이의 산술 연산 
__1.5.4 넘파이의 N차원 배열 
__1.5.5 브로드캐스트 
__1.5.6 원소 접근 
1.6 matplotlib 
__1.6.1 단순한 그래프 그리기 
__1.6.2 pyplot의 기능 
__1.6.3 이미지 표시하기
```

## 파일 설명

| 파일명           | 파일 용도                                         | 관련 절                    | 페이지 |
| :--------------- | :------------------------------------------------ | :------------------------- | :----- |
| hungry.py        | 단순한 문자열("I'm hungry!")을 출력합니다.        | 1.4.1 파일로 저장하기      | 34     |
| img_show.py      | 이미지를 화면에 보여줍니다.                       | 1.6.3 이미지 표시하기      | 44     |
| man.py           | 파이썬 클래스 정의 예제입니다.                    | 1.4.2 클래스               | 35     |
| simple_graph.py  | 그래프 그리기 예제입니다(sin_graph.py와 같음).    | 1.6.1 단순한 그래프 그리기 | 42     |
| sin_cos_graph.py | 사인(sin) 그래프와 코사인(cos) 그래프를 그립니다. | 1.6.2 pyplot의 기능        | 43     |
| sin_graph.py     | 사인(sin) 그래프를 그립니다.                      | 1.6.1 단순한 그래프 그리기 | 42     |

## 2장 퍼셉트론

퍼셉트론은 신경망(딥러닝)의 기원이 되는 알고리즘입니다.

> **목차**

```
2.1 퍼셉트론이란? 
2.2 단순한 논리 회로 
__2.2.1 AND 게이트 
__2.2.2 NAND 게이트와 OR 게이트 
2.3 퍼셉트론 구현하기 
__2.3.1 간단한 구현부터 
__2.3.2 가중치와 편향 도입 
__2.3.3 가중치와 편향 구현하기 
2.4 퍼셉트론의 한계 
__2.4.1 도전! XOR 게이트 
__2.4.2 선형과 비선형 
2.5 다층 퍼셉트론이 출동한다면 
__2.5.1 기존 게이트 조합하기 
__2.5.2 XOR 게이트 구현하기 
2.6 NAND에서 컴퓨터까지
```

## 파일 설명

| 파일명       | 파일 용도                                   | 관련 절                      | 페이지 |
| :----------- | :------------------------------------------ | :--------------------------- | :----- |
| and_gate.py  | 단층 퍼셉트론으로 AND 게이트를 구현합니다.  | 2.3.3 가중치와 편향 구현하기 | 53     |
| nand_gate.py | 단층 퍼셉트론으로 NAND 게이트를 구현합니다. | 2.3.3 가중치와 편향 구현하기 | 53     |
| or_gate.py   | 단층 퍼셉트론으로 OR 게이트를 구현합니다.   | 2.3.3 가중치와 편향 구현하기 | 53     |
| xor_gate.py  | 2층 퍼셉트론으로 XOR 게이트를 구현합니다.   | 2.5.2 XOR 게이트 구현하기    | 59     |

## 3장 신경망

퍼셉트론의 장점은 복잡한 함수도 표현할 수 있다는 것입니다. 단점은 가중치를 설정하는 작업(원하는 결과를 출력하도록 가중치 값을 적절히 정하는 작업)은 여전히 사람이 수동으로 한다는 것입니다. 

신경망은  가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습합니다. 이번 장에서는 신경망의 개요를 설명하고, 신경망이 입력 데이터가 무엇인지 식별하는 처리 과정을 자세히 알아봅니다. 

## 목차

```
3.1 퍼셉트론에서 신경망으로 
__3.1.1 신경망의 예 
__3.1.2 퍼셉트론 복습 
__3.1.3 활성화 함수의 등장 
3.2 활성화 함수 
__3.2.1 시그모이드 함수 
__3.2.2 계단 함수 구현하기 
__3.2.3 계단 함수의 그래프 
__3.2.4 시그모이드 함수 구현하기 
__3.2.5 시그모이드 함수와 계단 함수 비교 
__3.2.6 비선형 함수 
__3.2.7 ReLU 함수 
3.3 다차원 배열의 계산 
__3.3.1 다차원 배열 
__3.3.2 행렬의 내적 
__3.3.3 신경망의 내적 
3.4 3층 신경망 구현하기 
__3.4.1 표기법 설명 
__3.4.2 각 층의 신호 전달 구현하기 
__3.4.3 구현 정리 
3.5 출력층 설계하기 
__3.5.1 항등 함수와 소프트맥스 함수 구현하기 
__3.5.2 소프트맥스 함수 구현 시 주의점 
__3.5.3 소프트맥스 함수의 특징 
__3.5.4 출력층의 뉴런 수 정하기
3.6 손글씨 숫자 인식 
__3.6.1 MNIST 데이터셋 
__3.6.2 신경망의 추론 처리 
__3.6.3 배치 처리 
```

## 파일 설명

| 파일명                   | 파일 용도                                                    | 관련 절                                | 페이지 |
| :----------------------- | :----------------------------------------------------------- | :------------------------------------- | :----- |
| mnist_show.py            | MNIST 데이터셋을 읽어와 훈련 데이터 중 0번째 이미지를 화면에 출력합니다. | 3.6.1 손글씨 데이터셋                  | 99     |
| neuralnet_mnist.py       | 신경망으로 손글씨 숫자 그림을 추론합니다. 입력층, 은닉층1, 은닉층2, 출력층의 뉴런 수는 각각 784, 50, 100, 10입니다. | 3.6.2 신경망의 추론 처리               | 100    |
| neuralnet_mnist_batch.py | neuralnet_mnist.py에 배치 처리 기능을 더했습니다.            | 3.6.3 배치 처리                        | 104    |
| relu.py                  | ReLU 함수를 구현한 코드입니다.                               | 3.2.7 ReLU 함수                        | 76     |
| sample_weight.pkl        | 미리 학습해둔 가종치 매개변수의 값들입니다.                  | 3.6.2 신경망의 추론 처리               | 100    |
| sig_step_compare.py      | 시그모이드 함수와 계단 함수의 그래프 모양을 비교해봅니다.    | 3.2.5 시그모이드 함수와 계단 함수 비교 | 74     |
| sigmoid.py               | 시그모이드 함수를 구현한 코드입니다.                         | 3.2.4 시그모이드 함수 구현하기         | 72     |
| step_function.py         | 계단 함수를 구현한 코드입니다.                               | 3.2.3 계단 함수의 그래프               | 70     |

## 4장 신경망 학습

신경망 학습은 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻합니다.  신경망이 학습할 수 있도록 해주는 **지표**인 **손실 함수**를 소개합니다. 이 <u>손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것</u>이 학습의 목표입니다. 손실 함수의 값을 가급적 작게 만드는 기법으로, 함수의 기울기를 활용하는 **경사법**을 소개합니다.

## 목차

```
4.1 데이터에서 학습한다! 
__4.1.1 데이터 주도 학습 
__4.1.2 훈련 데이터와 시험 데이터 
4.2 손실 함수 
__4.2.1 평균 제곱 오차 
__4.2.2 교차 엔트로피 오차 
__4.2.3 미니배치 학습 
__4.2.4 (배치용) 교차 엔트로피 오차 구현하기 
__4.2.5 왜 손실 함수를 설정하는가? 
4.3 수치 미분 
__4.3.1 미분 
__4.3.2 수치 미분의 예 
__4.3.3 편미분 
4.4 기울기 
__4.4.1 경사법(경사 하강법) 
__4.4.2 신경망에서의 기울기 
4.5 학습 알고리즘 구현하기 
__4.5.1 2층 신경망 클래스 구현하기
__4.5.2 미니배치 학습 구현하기 
__4.5.3 시험 데이터로 평가하기
```

## 파일 설명

| 파일명                | 파일 용도                                                 | 관련 절                                                     | 페이지   |
| :-------------------- | :-------------------------------------------------------- | :---------------------------------------------------------- | :------- |
| gradient_1d.py        | 수치 미분으로 기울기를 구합니다.                          | 4.3.2 수치 미분의 예                                        | 125      |
| gradient_2d.py        | 수치 미분으로 기울기를 구합니다(두 편미분을 동시에 계산). | 4.4 기울기                                                  | 129      |
| gradient_method.py    | 경사하강법의 갱신 과정을 보여줍니다.                      | 4.4.1 경사법(경사 하강법)                                   | 132      |
| gradient_simplenet.py | simpleNet 클래스                                          | 4.4.2 신경망에서의 기울기                                   | 134      |
| train_neuralnet.py    | 미치배치 방식으로 학습하고 시험 데이터로 평가합니다.      | 4.5.2 미니배치 학습 구현하기 / 4.5.3 시험 데이터로 평가하기 | 141, 143 |
| two_layer_net.py      | 2층 신경망 클래스                                         | 4.5.1 2층 신경망 클래스 구현하기                            | 137      |

## 5장 오차역전파법

신경망의 학습에서 **가중치 매개변수의 기울기(정확히는 가중치 매개변수에 대한 손실 함수의 기울기)**는 <u>수치 미분</u>을 사용해 구했습니다. 수치 미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는 게 단점입니다.가중치 매개변수의 기울기를 효율적으로 계산하는 **‘오차역전파법backpropagation’**을 배워보겠습니다.
오차역전파법을 제대로 이해하는 방법은 두 가지가 있을 것입니다. 하나는 수식을 통한 것이고, 다른 하나는 계산 그래프를 통한 것입니다.

## 목차

```
5.1 계산 그래프 
__5.1.1 계산 그래프로 풀다 
__5.1.2 국소적 계산 
__5.1.3 왜 계산 그래프로 푸는가? 
5.2 연쇄법칙 
__5.2.1 계산 그래프에서의 역전파 
__5.2.2 연쇄법칙이란? 
__5.2.3 연쇄법칙과 계산 그래프 
5.3 역전파 
__5.3.1 덧셈 노드의 역전파 
__5.3.2 곱셈 노드의 역전파 
__5.3.3 사과 쇼핑의 예 
5.4 단순한 계층 구현하기 
__5.4.1 곱셈 계층 
__5.4.2 덧셈 계층 
5.5 활성화 함수 계층 구현하기 
__5.5.1 ReLU 계층 
__5.5.2 Sigmoid 계층 
5.6 Affine/Softmax 계층 구현하기 
__5.6.1 Affine 계층 
__5.6.2 배치용 Affine 계층 
__5.6.3 Softmax-with-Loss 계층 
5.7 오차역전파법 구현하기 
__5.7.1 신경망 학습의 전체 그림 
__5.7.2 오차역전파법을 적용한 신경망 구현하기 
__5.7.3 오차역전파법으로 구한 기울기 검증하기 
__5.7.4 오차역전파법을 사용한 학습 구현하기
```



## 6장 학습 관련 기술들

신경망 학습의 핵심 개념인 <u>가중치 매개변수의 최적값을 탐색</u>하는 **최적화 방법**, **가중치 매개변수 초깃값**, **하이퍼파라미터 설정 방법**, <u>오버피팅의 대응책</u>인 **가중치 감소와 드롭아웃 등의 정규화 방법**도 간략히 설명하고 구현해봅니다. 마지막으로 최근 많은 연구에서 사용하는 **배치 정규화**도 짧게 알아봅니다. 이러한 기법을 이용하면 <u>신경망(딥러닝) 학습의 효율과 정확도</u>를 높일 수 있습니다. 

## 목차

```
6.1 매개변수 갱신 
__6.1.1 모험가 이야기 
__6.1.2 확률적 경사 하강법(SGD) 
__6.1.3 SGD의 단점 
__6.1.4 모멘텀 
__6.1.5 AdaGrad 
__6.1.6 Adam 
__6.1.7 어느 갱신 방법을 이용할 것인가? 
__6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교 
6.2 가중치의 초깃값 
__6.2.1 초깃값을 0으로 하면? 
__6.2.2 은닉층의 활성화 분포 
__6.2.3 ReLU를 사용할 때의 가중치 초깃값 
__6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교 
6.3 배치 정규화 
__6.3.1 배치 정규화 알고리즘 
__6.3.2 배치 정규화의 효과 
6.4 바른 학습을 위해 
__6.4.1 오버피팅 
__6.4.2 가중치 감소 
__6.4.3 드롭아웃 
6.5 적절한 하이퍼파라미터 값 찾기 
__6.5.1 검증 데이터 
__6.5.2 하이퍼파라미터 최적화 
__6.5.3 하이퍼파라미터 최적화 구현하기 
```

## 파일 설명

| 파일명                              | 파일 용도                                                    | 관련 절                                        | 페이지 |
| :---------------------------------- | :----------------------------------------------------------- | :--------------------------------------------- | :----- |
| batch_norm_gradient_check.py        | 배치 정규화를 구현한 신경망의 오차역전파법 방식의 기울기 계산이 정확한지 확인합니다(기울기 확인). |                                                |        |
| batch_norm_test.py                  | MNIST 데이터셋 학습에 배치 정규화를 적용해봅니다.            | 6.3.2 배치 정규화의 효과                       | 212    |
| hyperparameter_optimization.py      | 무작위로 추출한 값부터 시작하여 두 하이퍼파라미터(가중치 감소 계수, 학습률)를 최적화해봅니다. | 6.5.3 하이퍼파라미터 최적화 구현하기           | 224    |
| optimizer_compare_mnist.py          | SGD, 모멘텀, AdaGrad, Adam의 학습 속도를 비교합니다.         | 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교     | 201    |
| optimizer_compare_naive.py          | SGD, 모멘텀, AdaGrad, Adam의 학습 패턴을 비교합니다.         | 6.1.7 어느 갱신 방법을 이용할 것인가?          | 200    |
| overfit_dropout.py                  | 일부러 오버피팅을 일으킨 후 드롭아웃(dropout)의 효과를 관찰합니다. | 6.4.3 드롭아웃                                 | 219    |
| overfit_weight_decay.py             | 일부러 오버피팅을 일으킨 후 가중치 감소(weight_decay)의 효과를 관찰합니다. | 6.4.1 오버피팅                                 | 215    |
| weight_init_activation_histogram.py | 활성화 함수로 시그모이드 함수를 사용하는 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 그려봅니다. | 6.2.2 은닉층의 활성화값 분포                   | 203    |
| weight_init_compare.py              | 가중치 초깃값(std=0.01, He, Xavier)에 따른 학습 속도를 비교합니다. | 6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교 | 209    |

## 7장 합성곱 신경망(CNN)

합성곱 신경망(convolutional neural network, CNN)은 이미지 인식과 음성 인식 등 다양한 곳에서 사용됩니다.

*합성곱(convolutional)은 공학과 물리학에서 널리 쓰이는 수학적 개념으로, 간단히 정의해보면 다음과 같습니다.
**“두 함수 중 하나를 반전(reverse), 이동(shift)시켜가며 나머지 함수와의 곱을 연이어 적분한다.”**.*

## 목차

```
7.1 전체 구조 
7.2 합성곱 계층 
__7.2.1 완전연결 계층의 문제점 
__7.2.2 합성곱 연산 
__7.2.3 패딩 
__7.2.4 스트라이드 
__7.2.5 3차원 데이터의 합성곱 연산 
__7.2.6 블록으로 생각하기 
__7.2.7 배치 처리 
7.3 풀링 계층 
__7.3.1 풀링 계층의 특징 
7.4 합성곱/풀링 계층 구현하기 
__7.4.1 4차원 배열 
__7.4.2 im2col로 데이터 전개하기 
__7.4.3 합성곱 계층 구현하기 
__7.4.4 풀링 계층 구현하기 
7.5 CNN 구현하기 
7.6 CNN 시각화하기 
__7.6.1 1번째 층의 가중치 시각화하기 
__7.6.2 층 깊이에 따른 추출 정보 변화 
7.7 대표적인 CNN 
__7.7.1 LeNet 
__7.7.2 AlexNet 
```

## 파일 설명

| 파일명              | 파일 용도                                                    | 관련 절                            | 페이지 |
| :------------------ | :----------------------------------------------------------- | :--------------------------------- | :----- |
| apply_filter.py     | lena_gray.png 파일에 필터를 적용합니다.                      |                                    |        |
| gradient_check.py   | SimpleCovNet이 기울기를 올바로 계산하는지 확인합니다.        |                                    |        |
| params.pkl          | 미리 학습된 가중치 값들입니다.                               |                                    |        |
| simple_convnet.py   | “Convolution-ReLU-Pooling-Affine-ReLU-Affine-Softmax” 순으로 흐르는 단순한 합성곱 신경망(CNN)입니다. | 7.5 CNN 구현하기                   | 251    |
| train_convnet.py    | SimpleConvNet으로 MNIST 데이터셋을 학습합니다.               | 7.5 CNN 구현하기                   | 254    |
| visualize_filter.py | 합성곱 1번째 층의 가중치를 학습 전과 후로 나눠 시각화해봅니다. 이미 학습된 가중치 값(params.pkl)을 읽어서 사용하므로 학습 과정은 생략됩니다. | 7.6.1 1번째 층의 가중치 시각화하기 | 254    |

## 8장 딥러닝

딥러닝은 층을 깊게 한 심층 신경망입니다. 딥러닝의 특징과 과제, 그리고 가능성을 살펴봅니다. 

## 목차

```
8.1 더 깊게 
__8.1.1 더 깊은 네트워크로 
__8.1.2 정확도를 더 높이려면 
__8.1.3 깊게 하는 이유 
8.2 딥러닝의 초기 역사 
__8.2.1 이미지넷 
__8.2.2 VGG 
__8.2.3 GoogLeNet 
__8.2.4 ResNet 
8.3 더 빠르게(딥러닝 고속화) 
__8.3.1 풀어야 할 숙제 
__8.3.2 GPU를 활용한 고속화 
__8.3.3 분산 학습 
__8.3.4 연산 정밀도와 비트 줄이기 
8.4 딥러닝의 활용 
__8.4.1 사물 검출 
__8.4.2 분할 
__8.4.3 사진 캡션 생성 
8.5 딥러닝의 미래 
__8.5.1 이미지 스타일(화풍) 변환 
__8.5.2 이미지 생성 
__8.5.3 자율 주행 
__8.5.4 Deep Q-Network(강화학습) 
```

## 파일 설명

| 파일명                  | 파일 용도                                                    | 관련 절                         | 페이지 |
| :---------------------- | :----------------------------------------------------------- | :------------------------------ | :----- |
| awesome_net.py          | 빈 파일입니다. 여기에 여러분만의 멋진 신경망을 구현해보세요! |                                 |        |
| deep_convnet.py         | [그림 8-1]의 깊은 신경망을 구현한 소스입니다.                | 8.1.1 더 깊은 신경망으로        | 262    |
| deep_convnet_params.pkl | deep_convnet.py용 학습된 가중치입니다.                       |                                 |        |
| half_float_network.py   | 수치 정밀도를 반정밀도(16비트)로 낮춰 계산하여 배정밀도(64비트)일 때와 정확도를 비교해본다. | 8.3.4 연산 정밀도와 비트 줄이기 | 278    |
| misclassified_mnist.py  | 이번 장에서 구현한 신경망이 인식에 실패한 손글씨 이미지들을 화면에 보여줍니다. | 8.1.1 더 깊은 신경망으로        | 263    |
| train_deepnet.py        | deep_convnet.py의 신경망을 학습시킵니다. 몇 시간은 걸리기 때문에 다른 코드에서는 미리 학습된 가중치인 deep_convnet_params.pkl을 읽어서 사용합니다. | 8.1.1 더 깊은 신경망으로        | 262    |